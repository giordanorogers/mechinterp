{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906a3879-20f8-44bd-8038-3f0634ae56ed",
   "metadata": {},
   "source": [
    "# What I'm Doing\n",
    "\n",
    "## 1. Prove necessity and sufficiency of Layer 37 chunks\n",
    "\n",
    "### 1.1 Necessity Sweep\n",
    "\n",
    "Ablate (zero-out) each block of Layer 37 separately:\n",
    "\n",
    "- Whole residual-stream -> logits\n",
    "- Each attention-head -> logits\n",
    "- Each MLP -> logits\n",
    "\n",
    "Record logit-difference (superset, top foil)\n",
    "\n",
    "### 1.2 Sufficiency Sweep\n",
    "\n",
    "Patch the same blocks clean -> corrupt while all other layers stay corrupted.\n",
    "\n",
    "- If a single head/MLP patch restores the answer, that slice is sufficient.\n",
    "\n",
    "### 1.3 Hydra Check\n",
    "\n",
    "After ablating the critical slice, look for backup heads lighting up.\n",
    "If backup appears, widen the slice; otherwise you've pinned the unique causal pathway.\n",
    "\n",
    "Outcome: a shortlist of critical sub-modules inside Layer 37\n",
    "\n",
    "## 2. Locate earlier retrieval heads feeding Layer 37\n",
    "\n",
    "1. Craft one-name probes (e.g. \"Brad Pitt is a...\") and run path-patch from earlier head to the Layer 37 slice you just isolated.\n",
    "2. Heads whose value vectors inject the correct profession label into that slice are retrieval heads.\n",
    "3. Verify by ablating the retrieval head -> Layer 37 path; the superset answer should break.\n",
    "\n",
    "Outcome: a two-hop path: Name tokens -> retrieval headsd (layers ~ 10-25) -> Layer 37 aggregator.\n",
    "\n",
    "## 3. Test the generality of the circuit\n",
    "\n",
    "1. Dense grid dataset - generate hundreds of entity pairs spanning >= 10 professions and 5 super-classes (artist, athlete, scientist ...)\n",
    "2. Repeat the same necessity/sufficiency tests.\n",
    "    - If the same heads fail across the grid, you have a general superset circuit.\n",
    "    - If the failure is class-specific, split the dataset and continue per class.\n",
    "3. Negative controls - include pairs whose superset is ambiguous or undefined. The circuit should stay silent or route elsewhere.\n",
    "\n",
    "Outcome: evidence that the circuit represents the logitcal rule \"f(x)=superset(profession(x))\" rather than memorized templates.\n",
    "\n",
    "## 4. Open thje black box of Layer 37 aggregator\n",
    "\n",
    "1. Linear sub-space probe - train a probe on the Layer 37 residual to predict the one-hot superset label.\n",
    "   - Low-rank => likely a single direction encoding \"artistness\".\n",
    "   - High-rank => multiple features; cluster them.\n",
    "2. Feature patching - patch only the \"artist\" direction from clean -> corrupt; if that alone fixes outputs, you have a feature-level explanation.\n",
    "3. Neuron search - run SAE or feature-visualization on the MLP to map specific neurons/features to superset classes.\n",
    "\n",
    "Outcome: a concrete mechanistic story:\n",
    "\n",
    "> \"Head 15.2 retrieves *profession* for each name -> its value is added to residual -> MLP 37.4 projects those values onto an 'artistness' direction; if either value is non-zero the logit for artist is boosted.\"\n",
    "\n",
    "## Practical guard-rails\n",
    "\n",
    "- Keep prompts short (name pairs + query) to avoid unrelated context features.\n",
    "- Match entity fame so retrieval confidence is uniform; otherwise uncertainty, not reasoning, may dominate activations.\n",
    "- Log all logits not just differences; a large negative swing in foils can masquerade as a positive causal effect.\n",
    "- Automate the sweeps--one DataFrame with columns: layer, head/MLP, metric-before, metric-after--and plot heatmaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31148d0-d5b6-4739-89b6-2df3b0d152fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb04b1-1382-4aab-8f7b-0c2d252d276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --progress -v https://github.com/giordanorogers/mechinterp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24920a4f-4c5c-4529-8619-24084e95a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/davidbau/baukit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ca841-0b0a-4294-80ab-de1b85d16727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"mechinterp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5c276-31e2-4cc0-b1e2-c35a6756b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "255827b7-47e4-44be-8bfb-a525e8eb2601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-02 10:05:23 __main__ INFO     torch.__version__='2.7.0', torch.version.cuda=None\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     27\u001b[39m transformers.logging.set_verbosity_error()\n\u001b[32m     29\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.__version__\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.version.cuda\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m logger.info(\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.is_available()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.cuda.device_count()\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m )\n\u001b[32m     33\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers.__version__\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_device_map\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/retrieval-env/lib/python3.11/site-packages/torch/cuda/__init__.py:544\u001b[39m, in \u001b[36mget_device_name\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_name\u001b[39m(device: Optional[_device_t] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    533\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[32m    534\u001b[39m \n\u001b[32m    535\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    542\u001b[39m \u001b[33;03m        str: the name of the device\u001b[39;00m\n\u001b[32m    543\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/retrieval-env/lib/python3.11/site-packages/torch/cuda/__init__.py:576\u001b[39m, in \u001b[36mget_device_properties\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_properties\u001b[39m(device: Optional[_device_t] = \u001b[38;5;28;01mNone\u001b[39;00m) -> _CudaDeviceProperties:\n\u001b[32m    565\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[32m    566\u001b[39m \n\u001b[32m    567\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    574\u001b[39m \u001b[33;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[32m    575\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[32m    577\u001b[39m     device = _get_device_index(device, optional=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m device < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device >= device_count():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/retrieval-env/lib/python3.11/site-packages/torch/cuda/__init__.py:363\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    359\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    361\u001b[39m     )\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    366\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")\n",
    "\n",
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "device_map = get_device_map(model_key, 32, n_gpus=1)\n",
    "print(device_map)\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa99c6e3-3bbf-4e07-a206-74b3fb15144e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-02 11:55:45 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/whoami-v2 HTTP/1.1\" 200 861\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Option A: Direct login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2999583c-d00a-4e04-8f28-6f43dc741a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "env.yml not found in /Users/giordanorogers/Documents/Code/mechinterp!\n",
      "Setting MODEL_ROOT=\"\". Models will now be downloaded to conda env cache, if not already there\n",
      "Other defaults are set to:\n",
      "    DATA_DIR = \"data\"\n",
      "    RESULTS_DIR = \"results\"\n",
      "    HPARAMS_DIR = \"hparams\"\n",
      "gpt2 not found in models/\n",
      "If not found in cache, model will be downloaded from HuggingFace to cache directory\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=\"gpt2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    #quantization_config = BitsAndBytesConfig(\n",
    "    #    load_in_4bit=True\n",
    "    #    #load_in_8bit=True\n",
    "    #)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c130329c-5a99-4c2f-a43f-f45191cd1a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Task: Find Common Attributes Between Two People\n",
      "You will be given two people's names. Your job is to determine if they share ANY common attribute from the list below.\n",
      "\n",
      "## Response Format:\n",
      "- If you find a match: \"Yes - [shared attribute] - [description of what they share]\"\n",
      "- If no match: \"No - [Person 1] and [Person 2] have nothing in common\"\n",
      "\n",
      "## Attributes to Consider:\n",
      "1. Same profession → \"Yes - [profession] - they are both [profession]\"\n",
      "2. Same school → \"Yes - [school] - they both graduated from [school]\"\n",
      "\n",
      "Q: Person Y and Person Z\n",
      "A: No - Person Y and Person Z have nothing in common.\n",
      "\n",
      "Q: Person W and Person X\n",
      "A: No - Person W and Person X have nothing in common.\n",
      "\n",
      "Q: Person C and Person D\n",
      "A: Yes - Doctor - they are both doctors.\n",
      "\n",
      "Q: Person E and Person F\n",
      "A: Yes - Boston University - they both graduated from Boston University.\n",
      "\n",
      "## Your turn, give your answer in a single line.\n"
     ]
    }
   ],
   "source": [
    "from src.probing.prompt import BiAssociationPrefix\n",
    "\n",
    "prefix_generator_cls = BiAssociationPrefix\n",
    "\n",
    "prefix_generator = prefix_generator_cls(\n",
    "    filter_attributes=[\n",
    "        #\"nationality\", \n",
    "        \"profession\", \n",
    "        \"school\"\n",
    "    ],\n",
    "    format = \"_3\"\n",
    ")\n",
    "\n",
    "prefix = prefix_generator.get_prefix()\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5a15c-1753-447a-a636-5989018b6c8f",
   "metadata": {},
   "source": [
    "## Prompt Pair\n",
    "\n",
    "Only the entity embeddings and their retrieval heads should differ, isolating weight-stored knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6165892f-8082-4efb-88d1-c0def5b32c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"Q: Pick the odd person out: Isaac Newton, Brad Pitt, Leonardo DiCaprio\\nA: I don't know. I don't know. I don't know. I don't know. I don't know. I don't know.\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from src.functional import generate_with_patch\n",
    "\n",
    "clean_prompt = \"Q: Pick the odd person out: Isaac Newton, Brad Pitt, Leonardo DiCaprio\\nA:\"\n",
    "corrupt_prompt = \"Q: Pick the odd person out: Isaac Newton, Albert Einstein, Leonardo DiCaprio\\nA:\"\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt,\n",
    "        inputs=clean_prompt,\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30\n",
    "    ),\n",
    "    indent=2,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "628134b6-fb3a-4730-a065-c212c8b136cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from src.tokens import prepare_input, find_token_range\n",
    "from src.probing.prompt import ProbingPrompt\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "def find_token_range(string, substring, tokenizer, offset_mapping, **kwargs):\n",
    "    \"\"\"\n",
    "    Return the start and end (inclusive) indices of the first occurrence\n",
    "    of search_term in input_str, or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    char_start = string.find(substring)\n",
    "    if char_start == -1:\n",
    "        return None, None\n",
    "    char_end = char_start + len(substring) - 1\n",
    "\n",
    "    token_start, token_end = None, None\n",
    "    for index, (token_char_start, token_char_end) in enumerate(offset_mapping):\n",
    "        if token_start is None:\n",
    "            if token_char_start <= char_start and token_char_end >= char_start:\n",
    "                token_start = index\n",
    "        if token_end is None:\n",
    "            if token_char_start <= char_end and token_char_end >= char_end:\n",
    "                token_end = index\n",
    "                break\n",
    "    return (token_start, token_end)\n",
    "\n",
    "def prepare_ooo_input(\n",
    "    mt: ModelandTokenizer,\n",
    "    entities = List[str],\n",
    "    prefix: str = \"Q: Pick the odd person out: \",\n",
    "    suffix: str = \"\\nA:\",\n",
    "    return_offsets_mapping: bool = False,\n",
    ") -> str:\n",
    "    prompt = f\"{prefix}{(', ').join(entities)}{suffix}\"\n",
    "\n",
    "    tokenized = prepare_input(\n",
    "        prompts=prompt,\n",
    "        tokenizer=mt,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    offset_mapping = tokenized[\"offset_mapping\"][0]\n",
    "\n",
    "    entity_ranges = tuple(\n",
    "        [\n",
    "            find_token_range(\n",
    "                string=prompt,\n",
    "                substring=entity,\n",
    "                tokenizer=mt,\n",
    "                offset_mapping=offset_mapping,\n",
    "            )\n",
    "            for entity in entities\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    query_range = find_token_range(\n",
    "        string=prompt,\n",
    "        substring=suffix,\n",
    "        tokenizer=mt,\n",
    "        offset_mapping=offset_mapping\n",
    "    )\n",
    "    query_token_idx = query_range[1]\n",
    "\n",
    "    tokenized = dict(\n",
    "        input_ids=tokenized[\"input_ids\"],\n",
    "        attention_mask=tokenized[\"attention_mask\"],\n",
    "    )\n",
    "    if return_offsets_mapping:\n",
    "        tokenized[\"offset_mapping\"] = [offset_mapping]\n",
    "\n",
    "    return ProbingPrompt(\n",
    "        prompt=prompt,\n",
    "        entities=entities,\n",
    "        model_key=mt.name.split(\"/\")[-1],\n",
    "        tokenized=tokenized,\n",
    "        entity_ranges=entity_ranges,\n",
    "        query_range=(query_token_idx, query_token_idx)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20cb171e-2ffa-4ae1-b230-0a71e4c0ecbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbingPrompt(prompt='Q: Pick the odd person out: Isaac Newton, Brad Pitt, Leonardo DiCaprio\\nA:', entities=['Isaac Newton', 'Brad Pitt', 'Leonardo DiCaprio'], model_key='gpt2', tokenized={'input_ids': tensor([[   48,    25, 12346,   262,  5629,  1048,   503,    25, 19068, 17321,\n",
       "            11,  8114, 10276,    11, 38083,  6031, 15610, 27250,   198,    32,\n",
       "            25]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='mps:0')}, entity_ranges=((8, 9), (11, 12), (14, 17)), query_range=(19, 19))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_ooo_input(\n",
    "    mt,\n",
    "    [\"Isaac Newton\", \"Brad Pitt\", \"Leonardo DiCaprio\"]\n",
    "    #[\"Tim Ferris\", \"Lex Fridman\", \"Dwarkesh Patel\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4629ad3f-ac69-4e94-bad2-199a39a8e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from src.models import ModelandTokenizer\n",
    "from src.functional import predict_next_token\n",
    "from src.probing.utils import get_lm_generated_answer\n",
    "\n",
    "def get_odd_entity_out(\n",
    "    mt: ModelandTokenizer,\n",
    "    entities: List[str],\n",
    "    prefix = \"Q: Pick the odd person out: \",\n",
    "    suffix = \"\\nA:\",\n",
    "    return_next_token_probs = True,\n",
    "    return_interesting_logits = True\n",
    "):\n",
    "    ooo_prompt = prepare_ooo_input(\n",
    "        mt=mt,\n",
    "        entities=entities,\n",
    "        prefix=prefix,\n",
    "        suffix=suffix,\n",
    "    )\n",
    "\n",
    "    answer = get_lm_generated_answer(\n",
    "        mt,\n",
    "        prompt=ooo_prompt\n",
    "    )\n",
    "    answer = answer.split(\"\\n\")[0]\n",
    "\n",
    "    if return_next_token_probs:\n",
    "        if return_interesting_logits:\n",
    "            entity_toks = [mt.tokenizer.encode(entity) for entity in entities]\n",
    "            first_toks = [name_toks[0] for name_toks in entity_toks]\n",
    "        return answer, predict_next_token(\n",
    "            mt=mt, inputs=ooo_prompt.prompt, k=5, preds_of_interest=first_toks\n",
    "        )\n",
    "    return answer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4382399e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39443, 30805, 36185]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents = [\"Isaac Newton\", \"Brad Pitt\", \"Leonardo DiCaprio\"]\n",
    "\n",
    "entity_toks = [mt.tokenizer.encode(entity) for entity in ents]\n",
    "\n",
    "first_toks = [name_toks[0] for name_toks in entity_toks]\n",
    "\n",
    "first_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a10a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "odd1out_dataset = {\n",
    "    \"actors_scientists\": [\n",
    "        {\n",
    "            \"entities\": [\"Isaac Newton\", \"Brad Pitt\", \"Leonardo DiCaprio\"],\n",
    "            \"target\": \"Isaac Newton\"\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Isaac Newton\", \"Albert Einstein\", \"Leonardo DiCaprio\"],\n",
    "            \"target\": \"Leonardo DiCaprio\"\n",
    "        }\n",
    "    ],\n",
    "    \"writers_athletes\": [\n",
    "        {\n",
    "            \"entities\": [\"Stephen King\", \"Mark Twain\", \"Usain Bolt\"],\n",
    "            \"target\": \"Usain Bolt\"\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Lionel Messi\", \"Mark Twain\", \"Usain Bolt\"],\n",
    "            \"target\": \"Mark Twain\"\n",
    "        }\n",
    "    ],\n",
    "    \"musicians_politician\": [\n",
    "        {\n",
    "            \"entities\": [\"Barack Obama\", \"Bob Dylan\", \"George Bush\"],\n",
    "            \"target\": \"Bob Dylan\"\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\"Barack Obama\", \"Bob Dylan\", \"John Lennon\"],\n",
    "            \"target\": \"Barack Obama\"\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e849240d-2deb-4ea2-a371-e879f8d4c3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Isaac Newton', 'Brad Pitt', 'Leonardo DiCaprio'] => Isaac Newton | ['\" I\"[314] (p=0.047, logit=-149.000)', '\" The\"[383] (p=0.047, logit=-149.000)', '\" It\"[632] (p=0.017, logit=-150.000)', '\" He\"[679] (p=0.017, logit=-150.000)', '\" You\"[921] (p=0.017, logit=-150.000)', \"{'Isa': {'logit': -160.0, 'prob': 7.911264106041926e-07, 'token_id': 39443}, 'Brad': {'logit': -160.0, 'prob': 7.911264106041926e-07, 'token_id': 30805}, 'Leon': {'logit': -160.0, 'prob': 7.911264106041926e-07, 'token_id': 36185}}\"]\n",
      "['Isaac Newton', 'Albert Einstein', 'Leonardo DiCaprio'] => Leonardo DiCaprio | ['\" I\"[314] (p=0.060, logit=-149.000)', '\" The\"[383] (p=0.022, logit=-150.000)', '\" It\"[632] (p=0.022, logit=-150.000)', '\" You\"[921] (p=0.022, logit=-150.000)', '\" If\"[1002] (p=0.022, logit=-150.000)', \"{'Isa': {'logit': -160.0, 'prob': 9.936701417245786e-07, 'token_id': 39443}, 'Albert': {'logit': -161.0, 'prob': 3.6555076121658203e-07, 'token_id': 42590}, 'Leon': {'logit': -160.0, 'prob': 9.936701417245786e-07, 'token_id': 36185}}\"]\n",
      "['Stephen King', 'Mark Twain', 'Usain Bolt'] => Usain Bolt | ['\" I\"[314] (p=0.089, logit=-150.000)', '\" The\"[383] (p=0.033, logit=-151.000)', '\" It\"[632] (p=0.033, logit=-151.000)', '\" You\"[921] (p=0.033, logit=-151.000)', '\" That\"[1320] (p=0.033, logit=-151.000)', \"{'Stephen': {'logit': -160.0, 'prob': 4.0407512642559595e-06, 'token_id': 24920}, 'Mark': {'logit': -161.0, 'prob': 1.4865094044580474e-06, 'token_id': 9704}, 'Us': {'logit': -164.0, 'prob': 7.400894475040332e-08, 'token_id': 5842}}\"]\n",
      "['Lionel Messi', 'Mark Twain', 'Usain Bolt'] => Mark Twain | ['\" I\"[314] (p=0.045, logit=-146.000)', '\" The\"[383] (p=0.045, logit=-146.000)', '\" It\"[632] (p=0.045, logit=-146.000)', '\" You\"[921] (p=0.045, logit=-146.000)', '\" We\"[775] (p=0.017, logit=-147.000)', \"{'L': {'logit': -155.0, 'prob': 5.587879968516063e-06, 'token_id': 43}, 'Mark': {'logit': -156.0, 'prob': 2.0556660729198484e-06, 'token_id': 9704}, 'Us': {'logit': -159.0, 'prob': 1.0234558089905477e-07, 'token_id': 5842}}\"]\n",
      "['Barack Obama', 'Bob Dylan', 'George Bush'] => Bob Dylan | ['\" I\"[314] (p=0.077, logit=-151.000)', '\" The\"[383] (p=0.028, logit=-152.000)', '\" It\"[632] (p=0.028, logit=-152.000)', '\" You\"[921] (p=0.028, logit=-152.000)', '\" That\"[1320] (p=0.028, logit=-152.000)', \"{'Bar': {'logit': -163.0, 'prob': 4.750455104840512e-07, 'token_id': 10374}, 'Bob': {'logit': -161.0, 'prob': 3.5101379580737557e-06, 'token_id': 18861}, 'George': {'logit': -161.0, 'prob': 3.5101379580737557e-06, 'token_id': 20191}}\"]\n",
      "['Barack Obama', 'Bob Dylan', 'John Lennon'] => Barack Obama | ['\" I\"[314] (p=0.057, logit=-150.000)', '\" The\"[383] (p=0.057, logit=-150.000)', '\" It\"[632] (p=0.021, logit=-151.000)', '\" He\"[679] (p=0.021, logit=-151.000)', '\" You\"[921] (p=0.021, logit=-151.000)', \"{'Bar': {'logit': -162.0, 'prob': 3.4839297313737916e-07, 'token_id': 10374}, 'Bob': {'logit': -160.0, 'prob': 2.5742951947904658e-06, 'token_id': 18861}, 'John': {'logit': -159.0, 'prob': 6.997660420893226e-06, 'token_id': 7554}}\"]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "limit = 100\n",
    "results = {}\n",
    "\n",
    "for professions in odd1out_dataset.items():\n",
    "    logger.info(\"-\" * 10 + f\" {professions[0]} \" + \"-\" * 10)\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    counter = 0\n",
    "    ooo_results = []\n",
    "    \n",
    "    for ent_targ in professions[1]:\n",
    "        query_entities = ent_targ['entities']\n",
    "        target = ent_targ['target']\n",
    "\n",
    "        answer, next_tok_probs = get_odd_entity_out(\n",
    "            mt=mt,\n",
    "            entities=query_entities,\n",
    "        )\n",
    "\n",
    "        next_tok_print = [str(pred) for pred in next_tok_probs[0]]\n",
    "        print(f\"{query_entities} => {target} | {next_tok_print}\")\n",
    "\n",
    "        ooo_results.append({\n",
    "            \"query_entities\": query_entities,\n",
    "            \"target\": target,\n",
    "            \"model_answer\": answer,\n",
    "            \"next_tok_probs\": next_tok_probs[0]\n",
    "        })\n",
    "\n",
    "        targets.append(target)\n",
    "        #print(next_tok_probs[0])\n",
    "\n",
    "        processed_tokens = []\n",
    "        for item in next_tok_probs[0]:\n",
    "            if hasattr(item, 'token'):\n",
    "                processed_tokens.append(item.token)\n",
    "            elif isinstance(item, dict):\n",
    "                processed_tokens.extend(list(item.keys()))\n",
    "        predictions.append(processed_tokens)\n",
    "\n",
    "        counter += 1\n",
    "        if counter >= limit:\n",
    "            break\n",
    "\n",
    "    results[professions[0]] = {\n",
    "        \"results\": ooo_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcede7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[' I', ' The', ' It', ' You', ' That', 'Bar', 'Bob', 'George'],\n",
       " [' I', ' The', ' It', ' He', ' You', 'Bar', 'Bob', 'John']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092280a",
   "metadata": {},
   "source": [
    "- Find token indices for first names\n",
    "    - Needs to get returned by `get_odd_entity_out` and added to the ooo_results\n",
    "- Find logit for each of those tokens in the predictions\n",
    "\n",
    "\n",
    "So now I need to find the token indices for all the first names.\n",
    "Then I will check for each prediction what the logit is for the correct first name token.\n",
    "And I will also check the difference between the first name token and the incorrect first name tokens.\n",
    "The difference between the correct first name token and the highest incorrect first name token will be the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181ce14",
   "metadata": {},
   "source": [
    "## 3. Define a metric\n",
    "\n",
    "For any model output, grab the logit for the correct answer, subtract the logit for the main rival. That single number -- the logit difference -- tells you how confidently the model picks the right odd one out.\n",
    "\n",
    "---\n",
    "\n",
    "Margin metric -- logit of the correct name minus the highest logit among the two wrong names.\n",
    "- Moves smoothly as you ablate or patch slices (helpful for localization)\n",
    "- Tells how far model is from flipping\n",
    "- Avoids picking arbitrary \"rival\" in advance\n",
    "\n",
    "```python\n",
    "def logit_margin(logits, correct_id, wrong_ids):\n",
    "    wrong_max = logits[0, -1, wrong_ids].max()\n",
    "    return (logits[0, -1, correct_id] - wrong_max).item()\n",
    "```\n",
    "\n",
    "- wrong_ids is a list of the two other name-token IDs\n",
    "- For every sweep you store that single float\n",
    "- When the margin foes from +3 to -2 you know the slice you just modified is pivotal\n",
    "\n",
    "Since names use two tokens\n",
    "- Compute the margin on the first token of each name (that token is unique enough for \"Einstein\", \"DiCaprio\")\n",
    "- Or, add the logits of both tokens for each name before taking the margin; results rarely differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177d5f6",
   "metadata": {},
   "source": [
    "## 4. Write three tiny hook helpers\n",
    "\n",
    "- ablate_layer: zeros a slice\n",
    "- patch_layer: pastes in stored activations\n",
    "- capture_layer copies activations into a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dbff94",
   "metadata": {},
   "source": [
    "## 5. Run the model once on each prompt\n",
    "\n",
    "Record the baseline logit differences.\n",
    "These baselines show you the gap you'll try to destory (necessity) or restore (sufficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab8e86",
   "metadata": {},
   "source": [
    "## 6. Necessity sweep\n",
    "\n",
    "Loop over layers. At every layer, zero the residual stream only at the swapped-name position and rerun the clean prompt.\n",
    "If the logit difference collapses at Layer 37, that layer is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a336fde",
   "metadata": {},
   "source": [
    "## 7. Sufficiency sweep\n",
    "\n",
    "First, save the clean activations for every layer at the swapped position.\n",
    "Then run the corrupt prompt. One layer at a time, paste the clean activation back in.\n",
    "The first layer that flips the answer back marks where the correct signal first becomes sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eaa194",
   "metadata": {},
   "source": [
    "## 8. Pick critical layers\n",
    "\n",
    "## 9. Zoom inside each critical layer\n",
    "\n",
    "## 10. Path Patch\n",
    "\n",
    "## 11. Backup-head test\n",
    "\n",
    "## 12. Generalization Grid\n",
    "\n",
    "## 13. Visualize\n",
    "\n",
    "## 14. Write the report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dfc76c-6fb2-4da5-8a10-656e8f374bec",
   "metadata": {},
   "source": [
    "## Coarse Sweep\n",
    "\n",
    "Patch residual stream at the two name positions across layers.\n",
    "These positions now contain all profession evidence. If patching either name from corrupt -> clean restores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74daee-ca25-4153-abc1-e643e249fb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465ae91-c595-49cd-b50f-bb47a9c81266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
