{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906a3879-20f8-44bd-8038-3f0634ae56ed",
   "metadata": {},
   "source": [
    "# What I'm Doing\n",
    "\n",
    "## 1. Prove necessity and sufficiency of Layer 37 chunks\n",
    "\n",
    "### 1.1 Necessity Sweep\n",
    "\n",
    "Ablate (zero-out) each block of Layer 37 separately:\n",
    "\n",
    "- Whole residual-stream -> logits\n",
    "- Each attention-head -> logits\n",
    "- Each MLP -> logits\n",
    "\n",
    "Record logit-difference (superset, top foil)\n",
    "\n",
    "### 1.2 Sufficiency Sweep\n",
    "\n",
    "Patch the same blocks clean -> corrupt while all other layers stay corrupted.\n",
    "\n",
    "- If a single head/MLP patch restores the answer, that slice is sufficient.\n",
    "\n",
    "### 1.3 Hydra Check\n",
    "\n",
    "After ablating the critical slice, look for backup heads lighting up.\n",
    "If backup appears, widen the slice; otherwise you've pinned the unique causal pathway.\n",
    "\n",
    "Outcome: a shortlist of critical sub-modules inside Layer 37\n",
    "\n",
    "## 2. Locate earlier retrieval heads feeding Layer 37\n",
    "\n",
    "1. Craft one-name probes (e.g. \"Brad Pitt is a...\") and run path-patch from earlier head to the Layer 37 slice you just isolated.\n",
    "2. Heads whose value vectors inject the correct profession label into that slice are retrieval heads.\n",
    "3. Verify by ablating the retrieval head -> Layer 37 path; the superset answer should break.\n",
    "\n",
    "Outcome: a two-hop path: Name tokens -> retrieval headsd (layers ~ 10-25) -> Layer 37 aggregator.\n",
    "\n",
    "## 3. Test the generality of the circuit\n",
    "\n",
    "1. Dense grid dataset - generate hundreds of entity pairs spanning >= 10 professions and 5 super-classes (artist, athlete, scientist ...)\n",
    "2. Repeat the same necessity/sufficiency tests.\n",
    "    - If the same heads fail across the grid, you have a general superset circuit.\n",
    "    - If the failure is class-specific, split the dataset and continue per class.\n",
    "3. Negative controls - include pairs whose superset is ambiguous or undefined. The circuit should stay silent or route elsewhere.\n",
    "\n",
    "Outcome: evidence that the circuit represents the logitcal rule \"f(x)=superset(profession(x))\" rather than memorized templates.\n",
    "\n",
    "## 4. Open thje black box of Layer 37 aggregator\n",
    "\n",
    "1. Linear sub-space probe - train a probe on the Layer 37 residual to predict the one-hot superset label.\n",
    "   - Low-rank => likely a single direction encoding \"artistness\".\n",
    "   - High-rank => multiple features; cluster them.\n",
    "2. Feature patching - patch only the \"artist\" direction from clean -> corrupt; if that alone fixes outputs, you have a feature-level explanation.\n",
    "3. Neuron search - run SAE or feature-visualization on the MLP to map specific neurons/features to superset classes.\n",
    "\n",
    "Outcome: a concrete mechanistic story:\n",
    "\n",
    "> \"Head 15.2 retrieves *profession* for each name -> its value is added to residual -> MLP 37.4 projects those values onto an 'artistness' direction; if either value is non-zero the logit for artist is boosted.\"\n",
    "\n",
    "## Practical guard-rails\n",
    "\n",
    "- Keep prompts short (name pairs + query) to avoid unrelated context features.\n",
    "- Match entity fame so retrieval confidence is uniform; otherwise uncertainty, not reasoning, may dominate activations.\n",
    "- Log all logits not just differences; a large negative swing in foils can masquerade as a positive causal effect.\n",
    "- Automate the sweeps--one DataFrame with columns: layer, head/MLP, metric-before, metric-after--and plot heatmaps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31148d0-d5b6-4739-89b6-2df3b0d152fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdb04b1-1382-4aab-8f7b-0c2d252d276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --progress -v https://github.com/giordanorogers/mechinterp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24920a4f-4c5c-4529-8619-24084e95a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/davidbau/baukit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ca841-0b0a-4294-80ab-de1b85d16727",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"mechinterp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5c276-31e2-4cc0-b1e2-c35a6756b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255827b7-47e4-44be-8bfb-a525e8eb2601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-02 13:44:08 __main__ INFO     torch.__version__='2.7.1+cu126', torch.version.cuda='12.6'\n",
      "2025-07-02 13:44:08 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-07-02 13:44:08 __main__ INFO     transformers.__version__='4.53.0'\n",
      "2025-07-02 13:44:11 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/gio/mechinterp, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-07-02 13:44:11 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/gio/mechinterp, stdin=None, shell=False, universal_newlines=False)\n",
      "{'model.embed_tokens': 7, 'model.norm': 7, 'model.rotary_emb': 7, 'lm_head': 7, 'model.layers.0': 0, 'model.layers.1': 1, 'model.layers.2': 2, 'model.layers.3': 3, 'model.layers.4': 4, 'model.layers.5': 5, 'model.layers.6': 6, 'model.layers.7': 7, 'model.layers.8': 0, 'model.layers.9': 1, 'model.layers.10': 2, 'model.layers.11': 3, 'model.layers.12': 4, 'model.layers.13': 5, 'model.layers.14': 6, 'model.layers.15': 7, 'model.layers.16': 0, 'model.layers.17': 1, 'model.layers.18': 2, 'model.layers.19': 3, 'model.layers.20': 4, 'model.layers.21': 5, 'model.layers.22': 6, 'model.layers.23': 7, 'model.layers.24': 0, 'model.layers.25': 1, 'model.layers.26': 2, 'model.layers.27': 3, 'model.layers.28': 4, 'model.layers.29': 5, 'model.layers.30': 6, 'model.layers.31': 7, 'model.layers.32': 0, 'model.layers.33': 1, 'model.layers.34': 2, 'model.layers.35': 3, 'model.layers.36': 4, 'model.layers.37': 5, 'model.layers.38': 6, 'model.layers.39': 7, 'model.layers.40': 0, 'model.layers.41': 1, 'model.layers.42': 2, 'model.layers.43': 3, 'model.layers.44': 4, 'model.layers.45': 5, 'model.layers.46': 6, 'model.layers.47': 7, 'model.layers.48': 0, 'model.layers.49': 1, 'model.layers.50': 2, 'model.layers.51': 3, 'model.layers.52': 4, 'model.layers.53': 5, 'model.layers.54': 6, 'model.layers.55': 7, 'model.layers.56': 0, 'model.layers.57': 1, 'model.layers.58': 2, 'model.layers.59': 3, 'model.layers.60': 4, 'model.layers.61': 5, 'model.layers.62': 6, 'model.layers.63': 7, 'model.layers.64': 0, 'model.layers.65': 1, 'model.layers.66': 2, 'model.layers.67': 3, 'model.layers.68': 4, 'model.layers.69': 5, 'model.layers.70': 6, 'model.layers.71': 7, 'model.layers.72': 0, 'model.layers.73': 1, 'model.layers.74': 2, 'model.layers.75': 3, 'model.layers.76': 4, 'model.layers.77': 5, 'model.layers.78': 6, 'model.layers.79': 7}\n",
      "/disk/u/gio/mechinterp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")\n",
    "\n",
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "device_map = get_device_map(model_key, 32, n_gpus=8)\n",
    "print(device_map)\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa99c6e3-3bbf-4e07-a206-74b3fb15144e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-02 11:55:45 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/whoami-v2 HTTP/1.1\" 200 861\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Option A: Direct login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b422778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/disk/u/gio/mechinterp'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "os.chdir(\"./mechinterp\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2999583c-d00a-4e04-8f28-6f43dc741a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1359f56599a34be6834438170d853fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-02 13:46:33 src.models INFO     loaded model <models/meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device_map=device_map,\n",
    "    device_map=\"auto\",\n",
    "    #quantization_config = BitsAndBytesConfig(\n",
    "    #    load_in_4bit=True\n",
    "    #    #load_in_8bit=True\n",
    "    #)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5a15c-1753-447a-a636-5989018b6c8f",
   "metadata": {},
   "source": [
    "## Prompt Pair\n",
    "\n",
    "Only the entity embeddings and their retrieval heads should differ, isolating weight-stored knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6165892f-8082-4efb-88d1-c0def5b32c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"Q: Pick the odd person out: Isaac Newton, Albert Einstein, Leonardo DiCaprio\\nA: Leonardo DiCaprio. The other two are famous scientists, while DiCaprio is an actor.\\nQ: Pick the odd person out: Charles Darwin\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from src.functional import generate_with_patch\n",
    "\n",
    "clean_prompt = \"Q: Pick the odd person out: Isaac Newton, Brad Pitt, Leonardo DiCaprio\\nA:\"\n",
    "corrupt_prompt = \"Q: Pick the odd person out: Isaac Newton, Albert Einstein, Leonardo DiCaprio\\nA:\"\n",
    "print(json.dumps(\n",
    "    generate_with_patch(\n",
    "        mt=mt,\n",
    "        inputs=corrupt_prompt,\n",
    "        n_gen_per_prompt=1,\n",
    "        do_sample=False,\n",
    "        max_new_tokens=30\n",
    "    ),\n",
    "    indent=2,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "628134b6-fb3a-4730-a065-c212c8b136cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from src.tokens import prepare_input, find_token_range\n",
    "from src.probing.prompt import ProbingPrompt\n",
    "from src.models import ModelandTokenizer\n",
    "\n",
    "def find_token_range(string, substring, tokenizer, offset_mapping, **kwargs):\n",
    "    \"\"\"\n",
    "    Return the start and end (inclusive) indices of the first occurrence\n",
    "    of search_term in input_str, or (None, None) if not found.\n",
    "    \"\"\"\n",
    "    char_start = string.find(substring)\n",
    "    if char_start == -1:\n",
    "        return None, None\n",
    "    char_end = char_start + len(substring) - 1\n",
    "\n",
    "    token_start, token_end = None, None\n",
    "    for index, (token_char_start, token_char_end) in enumerate(offset_mapping):\n",
    "        if token_start is None:\n",
    "            if token_char_start <= char_start and token_char_end >= char_start:\n",
    "                token_start = index\n",
    "        if token_end is None:\n",
    "            if token_char_start <= char_end and token_char_end >= char_end:\n",
    "                token_end = index\n",
    "                break\n",
    "    return (token_start, token_end)\n",
    "\n",
    "def prepare_ooo_input(\n",
    "    mt: ModelandTokenizer,\n",
    "    entities = List[str],\n",
    "    prefix: str = \"Q: Pick the odd person out: \",\n",
    "    suffix: str = \"\\nA:\",\n",
    "    return_offsets_mapping: bool = False,\n",
    ") -> str:\n",
    "    prompt = f\"{prefix}{(',').join(entities)}{suffix}\"\n",
    "\n",
    "    tokenized = prepare_input(\n",
    "        prompts=prompt,\n",
    "        tokenizer=mt,\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "    offset_mapping = tokenized[\"offset_mapping\"][0]\n",
    "\n",
    "    entity_ranges = tuple(\n",
    "        [\n",
    "            find_token_range(\n",
    "                string=prompt,\n",
    "                substring=entity,\n",
    "                tokenizer=mt,\n",
    "                offset_mapping=offset_mapping,\n",
    "            )\n",
    "            for entity in entities\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    query_range = find_token_range(\n",
    "        string=prompt,\n",
    "        substring=suffix,\n",
    "        tokenizer=mt,\n",
    "        offset_mapping=offset_mapping\n",
    "    )\n",
    "    query_token_idx = query_range[1]\n",
    "\n",
    "    tokenized = dict(\n",
    "        input_ids=tokenized[\"input_ids\"],\n",
    "        attention_mask=tokenized[\"attention_mask\"],\n",
    "    )\n",
    "    if return_offsets_mapping:\n",
    "        tokenized[\"offset_mapping\"] = [offset_mapping]\n",
    "\n",
    "    return ProbingPrompt(\n",
    "        prompt=prompt,\n",
    "        entities=entities,\n",
    "        model_key=mt.name.split(\"/\")[-1],\n",
    "        tokenized=tokenized,\n",
    "        entity_ranges=entity_ranges,\n",
    "        query_range=(query_token_idx, query_token_idx)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20cb171e-2ffa-4ae1-b230-0a71e4c0ecbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbingPrompt(prompt='Q: Pick the odd person out: Isaac Newton,Brad Pitt,Leonardo DiCaprio\\nA:', entities=['Isaac Newton', 'Brad Pitt', 'Leonardo DiCaprio'], model_key='Llama-3.3-70B-Instruct', tokenized={'input_ids': tensor([[128000,     48,     25,  20305,    279,  10535,   1732,    704,     25,\n",
       "          42608,  21324,     11,  62881,  21823,     11,  73004,  21106,   7923,\n",
       "          13199,  10599,    198,     32,     25]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='cuda:0')}, entity_ranges=((9, 10), (11, 13), (14, 19)), query_range=(21, 21))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_ooo_input(\n",
    "    mt,\n",
    "    [\"Isaac Newton\", \"Brad Pitt\", \"Leonardo DiCaprio\"]\n",
    "    #[\"Tim Ferris\", \"Lex Fridman\", \"Dwarkesh Patel\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4629ad3f-ac69-4e94-bad2-199a39a8e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from src.models import ModelandTokenizer\n",
    "from src.functional import predict_next_token\n",
    "from src.probing.utils import get_lm_generated_answer\n",
    "\n",
    "def get_odd_entity_out(\n",
    "    mt: ModelandTokenizer,\n",
    "    entities: List[str],\n",
    "    prefix = \"Q: Pick the odd person out: \",\n",
    "    suffix = \"\\nA:\",\n",
    "    return_next_token_probs = True,\n",
    "    return_interesting_logits = True\n",
    "):\n",
    "    ooo_prompt = prepare_ooo_input(\n",
    "        mt=mt,\n",
    "        entities=entities,\n",
    "        prefix=prefix,\n",
    "        suffix=suffix,\n",
    "    )\n",
    "\n",
    "    answer = get_lm_generated_answer(\n",
    "        mt,\n",
    "        prompt=ooo_prompt\n",
    "    )\n",
    "    answer = answer.split(\"\\n\")[0]\n",
    "\n",
    "    if return_next_token_probs:\n",
    "        if return_interesting_logits:\n",
    "            entity_toks = [mt.tokenizer.encode(entity, add_special_tokens=False) for entity in entities]\n",
    "            first_toks = [name_toks[0] for name_toks in entity_toks]\n",
    "        return answer, predict_next_token(\n",
    "            mt=mt, inputs=ooo_prompt.prompt, k=5, preds_of_interest=first_toks\n",
    "        )\n",
    "    return answer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4382399e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42608, 17478, 66486]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents = [\" Isaac Newton\", \" Brad Pitt\", \" Leonardo DiCaprio\"]\n",
    "\n",
    "entity_toks = [mt.tokenizer.encode(entity, add_special_tokens=False) for entity in ents]\n",
    "\n",
    "first_toks = [name_toks[0] for name_toks in entity_toks]\n",
    "\n",
    "first_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a10a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "odd1out_dataset = {\n",
    "    \"actors_scientists\": [\n",
    "        {\n",
    "            \"entities\": [\" Isaac Newton\", \" Brad Pitt\", \" Leonardo DiCaprio\"],\n",
    "            \"target\": \" Isaac\"\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\" Isaac Newton\", \" Albert Einstein\", \" Leonardo DiCaprio\"],\n",
    "            \"target\": \" Leonardo\"\n",
    "        }\n",
    "    ],\n",
    "    \"writers_athletes\": [\n",
    "        {\n",
    "            \"entities\": [\" Stephen King\", \" Mark Twain\", \" Usain Bolt\"],\n",
    "            \"target\": \" Us\"\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\" Lionel Messi\", \" Mark Twain\", \" Usain Bolt\"],\n",
    "            \"target\": \" Mark\"\n",
    "        }\n",
    "    ],\n",
    "    \"musicians_politician\": [\n",
    "        {\n",
    "            \"entities\": [\" Barack Obama\", \" Bob Dylan\", \" George Bush\"],\n",
    "            \"target\": \" Bob\"\n",
    "        },\n",
    "        {\n",
    "            \"entities\": [\" Barack Obama\", \" Bob Dylan\", \" John Lennon\"],\n",
    "            \"target\": \" Barack\"\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e849240d-2deb-4ea2-a371-e879f8d4c3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-02 14:15:49 __main__ INFO     ---------- actors_scientists ----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Isaac Newton', ' Brad Pitt', ' Leonardo DiCaprio'] =>  Isaac\n",
      "['\" Isaac\"[42608] (p=0.559, logit=18.875)', '\" The\"[578] (p=0.076, logit=16.875)', '\" Leonardo\"[66486] (p=0.059, logit=16.625)', '\" Brad\"[17478] (p=0.059, logit=16.625)', '\" \"[220] (p=0.031, logit=16.000)', \"{' Isaac': {'logit': 18.875, 'prob': 0.5578497648239136, 'token_id': 42608}, ' Brad': {'logit': 16.625, 'prob': 0.05879693105816841, 'token_id': 17478}, ' Leonardo': {'logit': 16.625, 'prob': 0.05879693105816841, 'token_id': 66486}}\"]\n",
      "[' Isaac Newton', ' Albert Einstein', ' Leonardo DiCaprio'] =>  Leonardo\n",
      "['\" Leonardo\"[66486] (p=0.562, logit=19.625)', '\" The\"[578] (p=0.098, logit=17.875)', '\" Albert\"[17971] (p=0.041, logit=17.000)', '\" Di\"[7923] (p=0.036, logit=16.875)', '\" That\"[3011] (p=0.036, logit=16.875)', \"{' Isaac': {'logit': 16.75, 'prob': 0.03174770623445511, 'token_id': 42608}, ' Albert': {'logit': 17.0, 'prob': 0.040764860808849335, 'token_id': 17971}, ' Leonardo': {'logit': 19.625, 'prob': 0.5627415180206299, 'token_id': 66486}}\"]\n",
      "2025-07-02 14:15:51 __main__ INFO     ---------- writers_athletes ----------\n",
      "[' Stephen King', ' Mark Twain', ' Usain Bolt'] =>  Us\n",
      "['\" Us\"[4073] (p=0.543, logit=19.875)', '\" The\"[578] (p=0.155, logit=18.625)', '\" Mark\"[4488] (p=0.057, logit=17.625)', '\" That\"[3011] (p=0.031, logit=17.000)', '\" I\"[358] (p=0.024, logit=16.750)', \"{' Stephen': {'logit': 16.625, 'prob': 0.021022237837314606, 'token_id': 18587}, ' Mark': {'logit': 17.625, 'prob': 0.057144369930028915, 'token_id': 4488}, ' Us': {'logit': 19.875, 'prob': 0.5421707034111023, 'token_id': 4073}}\"]\n",
      "[' Lionel Messi', ' Mark Twain', ' Usain Bolt'] =>  Mark\n",
      "['\" Mark\"[4488] (p=0.746, logit=20.250)', '\" The\"[578] (p=0.069, logit=17.875)', '\" Us\"[4073] (p=0.042, logit=17.375)', '\" Lionel\"[84224] (p=0.018, logit=16.500)', '\" This\"[1115] (p=0.012, logit=16.125)', \"{' Lionel': {'logit': 16.5, 'prob': 0.017575033009052277, 'token_id': 84224}, ' Mark': {'logit': 20.25, 'prob': 0.7473094463348389, 'token_id': 4488}, ' Us': {'logit': 17.375, 'prob': 0.042160313576459885, 'token_id': 4073}}\"]\n",
      "2025-07-02 14:15:52 __main__ INFO     ---------- musicians_politician ----------\n",
      "[' Barack Obama', ' Bob Dylan', ' George Bush'] =>  Bob\n",
      "['\" George\"[10058] (p=0.373, logit=18.625)', '\" Bob\"[14596] (p=0.330, logit=18.500)', '\" The\"[578] (p=0.045, logit=16.500)', '\" \"[220] (p=0.031, logit=16.125)', '\" Barack\"[24448] (p=0.024, logit=15.875)', \"{' Barack': {'logit': 15.875, 'prob': 0.02388482168316841, 'token_id': 24448}, ' Bob': {'logit': 18.5, 'prob': 0.32971981167793274, 'token_id': 14596}, ' George': {'logit': 18.625, 'prob': 0.37362149357795715, 'token_id': 10058}}\"]\n",
      "[' Barack Obama', ' Bob Dylan', ' John Lennon'] =>  Barack\n",
      "['\" John\"[3842] (p=0.295, logit=18.875)', '\" Barack\"[24448] (p=0.295, logit=18.875)', '\" Bob\"[14596] (p=0.095, logit=17.750)', '\" The\"[578] (p=0.058, logit=17.250)', '\" \"[220] (p=0.051, logit=17.125)', \"{' Barack': {'logit': 18.875, 'prob': 0.2940203547477722, 'token_id': 24448}, ' Bob': {'logit': 17.75, 'prob': 0.09545443207025528, 'token_id': 14596}, ' John': {'logit': 18.875, 'prob': 0.2940203547477722, 'token_id': 3842}}\"]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "limit = 100\n",
    "results = {}\n",
    "\n",
    "for professions in odd1out_dataset.items():\n",
    "    logger.info(\"-\" * 10 + f\" {professions[0]} \" + \"-\" * 10)\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    counter = 0\n",
    "    ooo_results = []\n",
    "    \n",
    "    for ent_targ in professions[1]:\n",
    "        query_entities = ent_targ['entities']\n",
    "        target = ent_targ['target']\n",
    "\n",
    "        answer, next_tok_probs = get_odd_entity_out(\n",
    "            mt=mt,\n",
    "            entities=query_entities,\n",
    "        )\n",
    "\n",
    "        next_tok_print = [f\"{str(pred)}\" for pred in next_tok_probs[0]]\n",
    "        print(f\"{query_entities} => {target}\\n{next_tok_print}\")\n",
    "\n",
    "        ooo_results.append({\n",
    "            \"query_entities\": query_entities,\n",
    "            \"target\": target,\n",
    "            \"model_answer\": answer,\n",
    "            \"next_tok_probs\": next_tok_probs[0]\n",
    "        })\n",
    "\n",
    "        targets.append(target)\n",
    "        #print(next_tok_probs[0])\n",
    "\n",
    "        processed_tokens = []\n",
    "        for item in next_tok_probs[0]:\n",
    "            if hasattr(item, 'token'):\n",
    "                processed_tokens.append(item.token)\n",
    "            elif isinstance(item, dict):\n",
    "                processed_tokens.extend(list(item.keys()))\n",
    "        predictions.append(processed_tokens)\n",
    "\n",
    "        counter += 1\n",
    "        if counter >= limit:\n",
    "            break\n",
    "\n",
    "    results[professions[0]] = {\n",
    "        \"results\": ooo_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "019dded2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit differences:\n",
      "1: Target=' Isaac', Margin=2.250 (correct: 18.875, best_wrong: 16.625)\n",
      "2: Target=' Leonardo', Margin=2.625 (correct: 19.625, best_wrong: 17.000)\n",
      "3: Target=' Us', Margin=2.250 (correct: 19.875, best_wrong: 17.625)\n",
      "4: Target=' Mark', Margin=2.875 (correct: 20.250, best_wrong: 17.375)\n",
      "5: Target=' Bob', Margin=-0.125 (correct: 18.500, best_wrong: 18.625)\n",
      "6: Target=' Barack', Margin=0.000 (correct: 18.875, best_wrong: 18.875)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_toks_and_logits = []\n",
    "logit_diffs = []\n",
    "\n",
    "for result in results.items():\n",
    "    category_results = result[1]['results']  # Get all results for this category\n",
    "    \n",
    "    for ooo_result in category_results:  # Loop through ALL results in this category\n",
    "        entity_first_tok_preds = ooo_result['next_tok_probs'][-1]\n",
    "        target = ooo_result['target']\n",
    "        \n",
    "        toks_and_logits = {}\n",
    "        for item in entity_first_tok_preds.items():\n",
    "            token = item[0]\n",
    "            logit = item[1]['logit']\n",
    "            toks_and_logits[token] = logit\n",
    "        \n",
    "        all_toks_and_logits.append(toks_and_logits)\n",
    "        \n",
    "        # Get correct answer logit\n",
    "        correct_logit = toks_and_logits[target]\n",
    "        \n",
    "        # Get all other logits (incorrect answers)\n",
    "        incorrect_logits = [logit for token, logit in toks_and_logits.items() if token != target]\n",
    "        \n",
    "        # Get the highest incorrect logit\n",
    "        max_incorrect_logit = max(incorrect_logits)\n",
    "        \n",
    "        # Compute logit difference (margin)\n",
    "        logit_diff = correct_logit - max_incorrect_logit\n",
    "        \n",
    "        logit_diffs.append({\n",
    "            'correct_logit': correct_logit,\n",
    "            'max_incorrect_logit': max_incorrect_logit,\n",
    "            'logit_margin': logit_diff,\n",
    "            'target': target\n",
    "        })\n",
    "\n",
    "print(\"Logit differences:\")\n",
    "for i, diff in enumerate(logit_diffs):\n",
    "    print(f\"{i+1}: Target='{diff['target']}', Margin={diff['logit_margin']:.3f} \"\n",
    "          f\"(correct: {diff['correct_logit']:.3f}, best_wrong: {diff['max_incorrect_logit']:.3f})\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092280a",
   "metadata": {},
   "source": [
    "- Find token indices for first names\n",
    "    - Needs to get returned by `get_odd_entity_out` and added to the ooo_results\n",
    "- Find logit for each of those tokens in the predictions\n",
    "\n",
    "\n",
    "So now I need to find the token indices for all the first names.\n",
    "Then I will check for each prediction what the logit is for the correct first name token.\n",
    "And I will also check the difference between the first name token and the incorrect first name tokens.\n",
    "The difference between the correct first name token and the highest incorrect first name token will be the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181ce14",
   "metadata": {},
   "source": [
    "## 3. Define a metric\n",
    "\n",
    "For any model output, grab the logit for the correct answer, subtract the logit for the main rival. That single number -- the logit difference -- tells you how confidently the model picks the right odd one out.\n",
    "\n",
    "---\n",
    "\n",
    "Margin metric -- logit of the correct name minus the highest logit among the two wrong names.\n",
    "- Moves smoothly as you ablate or patch slices (helpful for localization)\n",
    "- Tells how far model is from flipping\n",
    "- Avoids picking arbitrary \"rival\" in advance\n",
    "\n",
    "```python\n",
    "def logit_margin(logits, correct_id, wrong_ids):\n",
    "    wrong_max = logits[0, -1, wrong_ids].max()\n",
    "    return (logits[0, -1, correct_id] - wrong_max).item()\n",
    "```\n",
    "\n",
    "- wrong_ids is a list of the two other name-token IDs\n",
    "- For every sweep you store that single float\n",
    "- When the margin foes from +3 to -2 you know the slice you just modified is pivotal\n",
    "\n",
    "Since names use two tokens\n",
    "- Compute the margin on the first token of each name (that token is unique enough for \"Einstein\", \"DiCaprio\")\n",
    "- Or, add the logits of both tokens for each name before taking the margin; results rarely differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177d5f6",
   "metadata": {},
   "source": [
    "## 4. Write three tiny hook helpers\n",
    "\n",
    "- ablate_layer: zeros a slice\n",
    "- patch_layer: pastes in stored activations\n",
    "- capture_layer copies activations into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ea454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_layer():\n",
    "    \"\"\" Zero a hidden state. \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dbff94",
   "metadata": {},
   "source": [
    "## 5. Run the model once on each prompt\n",
    "\n",
    "Record the baseline logit differences.\n",
    "These baselines show you the gap you'll try to destory (necessity) or restore (sufficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab8e86",
   "metadata": {},
   "source": [
    "## 6. Necessity sweep\n",
    "\n",
    "Loop over layers. At every layer, zero the residual stream only at the swapped-name position and rerun the clean prompt.\n",
    "If the logit difference collapses at Layer 37, that layer is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a336fde",
   "metadata": {},
   "source": [
    "## 7. Sufficiency sweep\n",
    "\n",
    "First, save the clean activations for every layer at the swapped position.\n",
    "Then run the corrupt prompt. One layer at a time, paste the clean activation back in.\n",
    "The first layer that flips the answer back marks where the correct signal first becomes sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eaa194",
   "metadata": {},
   "source": [
    "## 8. Pick critical layers\n",
    "\n",
    "## 9. Zoom inside each critical layer\n",
    "\n",
    "## 10. Path Patch\n",
    "\n",
    "## 11. Backup-head test\n",
    "\n",
    "## 12. Generalization Grid\n",
    "\n",
    "## 13. Visualize\n",
    "\n",
    "## 14. Write the report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dfc76c-6fb2-4da5-8a10-656e8f374bec",
   "metadata": {},
   "source": [
    "## Coarse Sweep\n",
    "\n",
    "Patch residual stream at the two name positions across layers.\n",
    "These positions now contain all profession evidence. If patching either name from corrupt -> clean restores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74daee-ca25-4153-abc1-e643e249fb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465ae91-c595-49cd-b50f-bb47a9c81266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
