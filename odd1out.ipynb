{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `main_token` has been saved to /disk/u/gio/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /disk/u/gio/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `main_token`\n"
     ]
    }
   ],
   "source": [
    "from nnsight import CONFIG\n",
    "NDIF_API = \"\"\n",
    "HF_TOKEN = \"\"\n",
    "\n",
    "CONFIG.set_default_api_key(NDIF_API)\n",
    "!huggingface-cli login --token {HF_TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nnsight\n",
    "from nnsight import LanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 8192)\n",
      "    (layers): ModuleList(\n",
      "      (0-79): 80 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "          (k_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=8192, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=8192, out_features=8192, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "          (up_proj): Linear(in_features=8192, out_features=28672, bias=False)\n",
      "          (down_proj): Linear(in_features=28672, out_features=8192, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((8192,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((8192,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((8192,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=8192, out_features=128256, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(\"meta-llama/Meta-Llama-3.1-70B\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Q: Pick the odd person out: Isaac Newton, Brad Pitt, Leonardo DiCaprio\\nA:\",\n",
    "    \"Q: Pick the odd person out: Isaac Newton, Albert Einstein, Leonardo DiCaprio\\nA:\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [\n",
    "    (\" Isaac\", (\" Brad\", \" Leonardo\")),\n",
    "    (\" Leonardo\", (\" Isaac\", \" Albert\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = model.tokenizer(prompts, return_tensors=\"pt\")[\"input_ids\"]\n",
    "corrupted_tokens = clean_tokens[\n",
    "    [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[model.tokenizer.decode(token) for token in clean_tokens]\n",
    "#[model.tokenizer.decode(token) for token in corrupted_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([42608], [17478, 66486]), ([66486], [42608, 17971])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_toks = []\n",
    "for i in range(len(answers)):\n",
    "    main_tok = []\n",
    "    other_toks = []\n",
    "    for j in range(2):\n",
    "        if j == 0:\n",
    "            tok_correct = model.tokenizer(answers[i][j], add_special_tokens=False)['input_ids']\n",
    "            #print(tok_correct)\n",
    "            main_tok.extend(tok_correct)\n",
    "        elif j == 1:\n",
    "            for k in range(2):\n",
    "               tok_other = model.tokenizer(answers[i][j][k], add_special_tokens=False)['input_ids']\n",
    "               #print(tok_other)\n",
    "               other_toks.extend(tok_other)\n",
    "    all_toks.append((main_tok, other_toks))\n",
    "all_toks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching Experiment\n",
    "\n",
    "Now we can run the actual patching intervention.\n",
    "\n",
    "We have two prompts, a \"clean\" one and a \"corrupted\" one.\n",
    "Intuitively, the model output for each of these prompts should be different: we'd expect the model to asnwer \" Isaac\" for the clean prompt and \" Leonardo\" for the corrupted prompt.\n",
    "\n",
    "In this experiment, we run the model with the clean prompt as an input and then (1) get each layer's output value (i.e., residual stream) and (2) calculate the logit difference between the correct and incorrect answers for this run. Next, we calculate the logit difference between the correct and incorrect answers for the corrupted prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Clean Run\n",
    "\n",
    "During this clean run, we collect the final output of each layer.\n",
    "We also record the logit difference in the final model output between the correct answer token \" Isaac\" and the incorrect tokens \" Brad\" and \" Leonardo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-03 12:04:23,655 906834fa-419c-43cd-bff8-a69fcf506e75 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2025-07-03 12:04:23,829 906834fa-419c-43cd-bff8-a69fcf506e75 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2025-07-03 12:04:30,713 906834fa-419c-43cd-bff8-a69fcf506e75 - RUNNING: Your job has started running.\n",
      "2025-07-03 12:04:30,984 906834fa-419c-43cd-bff8-a69fcf506e75 - COMPLETED: Your job has been completed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9010d105afa14cb0b1e028388e104413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading result:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_LAYERS = len(model.model.layers)\n",
    "\n",
    "with model.trace(prompts[0], remote=True) as tracer:\n",
    "    clean_tokens = tracer.invoker.inputs[0][0]['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
