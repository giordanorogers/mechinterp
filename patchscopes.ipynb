{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 10:57:27 __main__ INFO     torch.__version__='2.7.1+cu126', torch.version.cuda='12.6'\n",
      "2025-06-28 10:57:28 __main__ INFO     torch.cuda.is_available()=True, torch.cuda.device_count()=8, torch.cuda.get_device_name()='NVIDIA A100 80GB PCIe'\n",
      "2025-06-28 10:57:28 __main__ INFO     transformers.__version__='4.53.0'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "##################################################################\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "##################################################################\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")\n",
    "logger.info(\n",
    "    f\"{torch.cuda.is_available()=}, {torch.cuda.device_count()=}, {torch.cuda.get_device_name()=}\"\n",
    ")\n",
    "logger.info(f\"{transformers.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 10:57:40 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/gio/mechinterp, stdin=None, shell=False, universal_newlines=False)\n",
      "2025-06-28 10:57:40 git.cmd DEBUG    Popen(['git', 'version'], cwd=/disk/u/gio/mechinterp, stdin=None, shell=False, universal_newlines=False)\n",
      "x: 42\n",
      "y: hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model.embed_tokens': 7,\n",
       " 'model.norm': 7,\n",
       " 'model.rotary_emb': 7,\n",
       " 'lm_head': 7,\n",
       " 'model.layers.0': 0,\n",
       " 'model.layers.1': 1,\n",
       " 'model.layers.2': 2,\n",
       " 'model.layers.3': 3,\n",
       " 'model.layers.4': 4,\n",
       " 'model.layers.5': 5,\n",
       " 'model.layers.6': 6,\n",
       " 'model.layers.7': 7,\n",
       " 'model.layers.8': 0,\n",
       " 'model.layers.9': 1,\n",
       " 'model.layers.10': 2,\n",
       " 'model.layers.11': 3,\n",
       " 'model.layers.12': 4,\n",
       " 'model.layers.13': 5,\n",
       " 'model.layers.14': 6,\n",
       " 'model.layers.15': 7,\n",
       " 'model.layers.16': 0,\n",
       " 'model.layers.17': 1,\n",
       " 'model.layers.18': 2,\n",
       " 'model.layers.19': 3,\n",
       " 'model.layers.20': 4,\n",
       " 'model.layers.21': 5,\n",
       " 'model.layers.22': 6,\n",
       " 'model.layers.23': 7,\n",
       " 'model.layers.24': 0,\n",
       " 'model.layers.25': 1,\n",
       " 'model.layers.26': 2,\n",
       " 'model.layers.27': 3,\n",
       " 'model.layers.28': 4,\n",
       " 'model.layers.29': 5,\n",
       " 'model.layers.30': 6,\n",
       " 'model.layers.31': 7,\n",
       " 'model.layers.32': 0,\n",
       " 'model.layers.33': 1,\n",
       " 'model.layers.34': 2,\n",
       " 'model.layers.35': 3,\n",
       " 'model.layers.36': 4,\n",
       " 'model.layers.37': 5,\n",
       " 'model.layers.38': 6,\n",
       " 'model.layers.39': 7,\n",
       " 'model.layers.40': 0,\n",
       " 'model.layers.41': 1,\n",
       " 'model.layers.42': 2,\n",
       " 'model.layers.43': 3,\n",
       " 'model.layers.44': 4,\n",
       " 'model.layers.45': 5,\n",
       " 'model.layers.46': 6,\n",
       " 'model.layers.47': 7,\n",
       " 'model.layers.48': 0,\n",
       " 'model.layers.49': 1,\n",
       " 'model.layers.50': 2,\n",
       " 'model.layers.51': 3,\n",
       " 'model.layers.52': 4,\n",
       " 'model.layers.53': 5,\n",
       " 'model.layers.54': 6,\n",
       " 'model.layers.55': 7,\n",
       " 'model.layers.56': 0,\n",
       " 'model.layers.57': 1,\n",
       " 'model.layers.58': 2,\n",
       " 'model.layers.59': 3,\n",
       " 'model.layers.60': 4,\n",
       " 'model.layers.61': 5,\n",
       " 'model.layers.62': 6,\n",
       " 'model.layers.63': 7,\n",
       " 'model.layers.64': 0,\n",
       " 'model.layers.65': 1,\n",
       " 'model.layers.66': 2,\n",
       " 'model.layers.67': 3,\n",
       " 'model.layers.68': 4,\n",
       " 'model.layers.69': 5,\n",
       " 'model.layers.70': 6,\n",
       " 'model.layers.71': 7,\n",
       " 'model.layers.72': 0,\n",
       " 'model.layers.73': 1,\n",
       " 'model.layers.74': 2,\n",
       " 'model.layers.75': 3,\n",
       " 'model.layers.76': 4,\n",
       " 'model.layers.77': 5,\n",
       " 'model.layers.78': 6,\n",
       " 'model.layers.79': 7}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.utils.training_utils import get_device_map\n",
    "\n",
    "model_key = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "device_map = get_device_map(model_key, 32, n_gpus=8)\n",
    "device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/disk/u/gio/mechinterp'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d91177932fe646249fcbb29bf4df68f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-28 10:58:45 src.models INFO     loaded model <models/meta-llama/Llama-3.3-70B-Instruct> | size: 134570.516 MB | dtype: torch.bfloat16 | device: cuda:7\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=model_key,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cla' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_hs\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mcla\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'cla' is not defined"
     ]
    }
   ],
   "source": [
    "from src.functional import get_hs\n",
    "\n",
    "class Patchscope:\n",
    "    def __init__(self, mt):\n",
    "        self.model = mt.model\n",
    "        self.tokenizer = mt.tokenizer\n",
    "\n",
    "    def run(\n",
    "        self, \n",
    "        source_prompt,\n",
    "        target_prompt,\n",
    "        source_layer_idx,\n",
    "        target_layer_idx,\n",
    "        target_token_idx\n",
    "    ):\n",
    "        # Feed source prompt to model and capture hidden state\n",
    "        source_inputs = self.tokenizer(source_prompt, return_tensors=\"pt\")\n",
    "\n",
    "        # Register a forward hook on the specified layer\n",
    "        source_hook_handle = self.model.model.layers[source_layer_idx].register_forward_hook(get_hidden_state_hook)\n",
    "\n",
    "        with torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
